{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "480d22f7-43ae-4d04-a35c-99e641f090c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import camelot\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70facb4b-df6d-4f0a-a5af-d6def60816c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_continued_tables(tables, threshold):\n",
    "\n",
    "    continued_tables = {}\n",
    "    previous_table = False\n",
    "    group_counter = 0\n",
    "\n",
    "    # typical height of a pdf is 842 points and bottom margins are anywhere between 56 and 85 points\n",
    "    # therefore, accounting for margins, 792\n",
    "    page_height = 792\n",
    "\n",
    "    # iterate over the tables\n",
    "    for i, table in enumerate(tables):\n",
    "\n",
    "        # if a previous table exists (remember, we start with this as false)\n",
    "        # and the previous table was on the previous page\n",
    "        # and the number of columns of both tables is the same\n",
    "        if previous_table and table.page == previous_table.page + 1 and len(table.cols) == len(previous_table.cols):\n",
    "\n",
    "            # get the bottom coordinate of the previous table\n",
    "            # note that for pdfs the origin (0, 0) typically starts from the bottom-left corner of the page,\n",
    "            # with the y-coordinate increasing as you move upwards\n",
    "            # this is why for {x0, y0, x1, y1} we need the y0 as the bottom\n",
    "            previous_table_bottom = previous_table._bbox[1]\n",
    "\n",
    "            # get the top coordinate of the current table\n",
    "            # for {x0, y0, x1, y1} we need the y1 as the top\n",
    "            current_table_top = table._bbox[3]\n",
    "\n",
    "            # if the previous table ends in the last 15% of the page and the current table starts in the first 15% of the page\n",
    "            if previous_table_bottom < (threshold / 100) * page_height and current_table_top > (1 - threshold / 100) * page_height:\n",
    "\n",
    "                # if we don't have started this group of tables\n",
    "                if (continued_tables.get(group_counter) is None):\n",
    "\n",
    "                    # start by adding the first table\n",
    "                    continued_tables[group_counter] = [previous_table]\n",
    "\n",
    "                # add any of the sunsequent tables to the group\n",
    "                continued_tables[group_counter].append(table)\n",
    "\n",
    "            # if this is not a continuation of the previous table\n",
    "            else:\n",
    "\n",
    "                # increment the group number\n",
    "                group_counter += 1;\n",
    "\n",
    "        # if this is not a continuation of the previous table\n",
    "        else:\n",
    "\n",
    "            # increment the group number\n",
    "            group_counter += 1;\n",
    "\n",
    "        # the current table becomes the previous table for the next iteration\n",
    "        previous_table = table\n",
    "\n",
    "    # transform the dictionary into an array of arrays\n",
    "    continued_tables = [value for value in continued_tables.values()]\n",
    "\n",
    "    # return the combined tables\n",
    "    return continued_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a433aec-10cc-4aca-b236-b078b4ac486d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_to_json(table_data, table_info):\n",
    "    \"\"\"Convert table data to JSON format\"\"\"\n",
    "    if not table_data:\n",
    "        return {}\n",
    "    \n",
    "    # Create JSON structure\n",
    "    json_data = {\n",
    "        \"metadata\": {\n",
    "            \"source_file\": table_info[\"source_file\"],\n",
    "            \"page\": table_info[\"page\"],\n",
    "            \"table_order\": table_info[\"order\"],\n",
    "            \"total_rows\": len(table_data),\n",
    "            \"total_columns\": len(table_data[0]) if table_data else 0\n",
    "        },\n",
    "        \"headers\": [],\n",
    "        \"data\": []\n",
    "    }\n",
    "    \n",
    "    # Add headers (first row)\n",
    "    if len(table_data) > 0:\n",
    "        headers = [str(cell).strip() for cell in table_data[0]]\n",
    "        \n",
    "        # Replace first 3 headers with fixed names\n",
    "        if len(headers) >= 1:\n",
    "            headers[0] = \"STT\"\n",
    "        if len(headers) >= 2:\n",
    "            headers[1] = \"hang_hoa\"\n",
    "        if len(headers) >= 3:\n",
    "            headers[2] = \"yeu_cau_ky_thuat\"\n",
    "            \n",
    "        json_data[\"headers\"] = headers\n",
    "        \n",
    "        # Add data rows (skip header)\n",
    "        for i, row in enumerate(table_data[1:], 1):\n",
    "            row_dict = {}\n",
    "            for j, cell in enumerate(row):\n",
    "                # Use header as key, fallback to column index if header is empty\n",
    "                key = json_data[\"headers\"][j] if j < len(json_data[\"headers\"]) and json_data[\"headers\"][j] else f\"column_{j}\"\n",
    "                row_dict[key] = str(cell).strip()\n",
    "            \n",
    "            json_data[\"data\"].append({\n",
    "                \"row_index\": i,\n",
    "                \"values\": row_dict\n",
    "            })\n",
    "    \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa48222e-ed6c-4ba7-8193-f500316d0f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_biggest_table(pdf_path, threshold):\n",
    "    tables = camelot.read_pdf(pdf_path, flavor = 'lattice', pages = 'all')\n",
    "    continued_tables = get_continued_tables(tables, threshold)\n",
    "\n",
    "    # get the name of the PDF file we are processing (without the extension)\n",
    "    pdf_file_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "\n",
    "    processed = []\n",
    "    all_table_jsons = []\n",
    "\n",
    "    # iterate over found tables\n",
    "    for i, table in enumerate(tables):\n",
    "\n",
    "        # if table was already processed as part of a group\n",
    "        if table in processed: continue\n",
    "\n",
    "        # check if the current table is a continued table\n",
    "        is_continued = any(table in sublist for sublist in continued_tables)\n",
    "\n",
    "        # collect all table data (current table + continued tables if any)\n",
    "        all_table_data = list(table.data)\n",
    "\n",
    "        # if the current table is a continued table, append all subsequent continued tables data\n",
    "        if is_continued:\n",
    "\n",
    "            # get the index of the group in \"continued_tables\" associated with the current table\n",
    "            group_index = next(index for index, sublist in enumerate(continued_tables) if table in sublist)\n",
    "\n",
    "            # iterate over the tables in said group and append their data\n",
    "            for continued_table in continued_tables[group_index]:\n",
    "\n",
    "                # skip the current table as it's already added\n",
    "                if continued_table == table or continued_table in processed: continue\n",
    "\n",
    "                # append the data of the continued table (skip header for subsequent tables)\n",
    "                all_table_data.extend(continued_table.data[1:] if len(continued_table.data) > 1 else [])\n",
    "\n",
    "                # keep track of processed tables\n",
    "                processed.append(continued_table)\n",
    "\n",
    "        # convert to JSON\n",
    "        table_info = {\n",
    "            \"source_file\": pdf_file_name,\n",
    "            \"page\": table.parsing_report['page'],\n",
    "            \"order\": table.parsing_report['order']\n",
    "        }\n",
    "        \n",
    "        json_data = table_to_json(all_table_data, table_info)\n",
    "        all_table_jsons.append(json_data)\n",
    "        \n",
    "        # mark current table as processed\n",
    "        processed.append(table)\n",
    "\n",
    "    # find the table with the most rows\n",
    "    if all_table_jsons:\n",
    "        largest_table = max(all_table_jsons, key=lambda x: x.get('metadata', {}).get('total_rows', 0))\n",
    "        \n",
    "        # return the JSON of the largest table\n",
    "        print(json.dumps(largest_table, ensure_ascii=False, indent=2))\n",
    "        return largest_table\n",
    "    else:\n",
    "        print(\"No tables found in the PDF.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bc6678-3bd6-4715-b4c2-19f01a8aee3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hello = get_biggest_table(\"D:/study/LammaIndex/documents/Chuong_V_Yeu_cau_ky_thuat.pdf\",15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7119be96-fc50-4537-ac75-b7ed77fe33ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = hello[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85ef3d1-9e16-43e0-b63e-b4ab792a92d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42238fb-e343-4492-be95-cd2f03b9577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "def clean_text(text):\n",
    "    \"\"\"Làm sạch text, loại bỏ ký tự xuống dòng thừa\"\"\"\n",
    "    return re.sub(r'\\n+', '', text.strip())\n",
    "\n",
    "def split_requirements(text):\n",
    "    \"\"\"Tách các yêu cầu dựa trên dấu gạch đầu dòng\"\"\"\n",
    "    requirements = []\n",
    "    lines = text.split('\\n')\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith('- '):\n",
    "            requirements.append(line[2:].strip())\n",
    "        elif line and not any(line.startswith(prefix) for prefix in ['- ']):\n",
    "            if requirements:\n",
    "                requirements[-1] += ' ' + line\n",
    "            else:\n",
    "                requirements.append(line)\n",
    "    return requirements\n",
    "\n",
    "def generate_random_key():\n",
    "    \"\"\"Tạo key random 5 ký tự từ UUID\"\"\"\n",
    "    return str(uuid.uuid4()).replace('-', '')[:5].upper()\n",
    "\n",
    "def convert_to_new_format(data):\n",
    "    result = []\n",
    "    current_product = None\n",
    "    current_category = None\n",
    "    \n",
    "    for item in data:\n",
    "        values = item['values']\n",
    "        stt_raw  = values['STT']\n",
    "        hang_hoa = clean_text(values['hang_hoa'])\n",
    "        yeu_cau = values['yeu_cau_ky_thuat']\n",
    "\n",
    "\n",
    "        stt = stt_raw.strip()\n",
    "\n",
    "        roman_pattern = r'^(VII|VIII|IX|X|XI|XII|I{1,3}|IV|V|VI)\\s+(.+)'\n",
    "        roman_match = re.match(roman_pattern, stt)\n",
    "        # Nếu STT là số La Mã (I, II, III...) thì đây là tên sản phẩm\n",
    "        hang_hoa_roman_match = re.match(roman_pattern, hang_hoa)\n",
    "        if roman_match and not hang_hoa and not yeu_cau:\n",
    "            if current_product:\n",
    "                result.append(current_product)\n",
    "            \n",
    "            roman_num = roman_match.group(1)  # Số La Mã\n",
    "            product_name = roman_match.group(2)  # Tên sản phẩm\n",
    "            \n",
    "            current_product = {\n",
    "                \"ten_san_pham\": product_name,\n",
    "                \"cac_muc\": []\n",
    "            }\n",
    "            current_category = None\n",
    "        elif hang_hoa_roman_match and not stt_raw and not yeu_cau:\n",
    "            if current_product:\n",
    "                result.append(current_product)\n",
    "            \n",
    "            roman_num = hang_hoa_roman_match.group(1)  # Số La Mã\n",
    "            product_name = hang_hoa_roman_match.group(2)  # Tên sản phẩm\n",
    "            \n",
    "            current_product = {\n",
    "                \"ten_san_pham\": product_name,\n",
    "                \"cac_muc\": []\n",
    "            }\n",
    "            current_category = None        \n",
    "        \n",
    "        elif stt in ['I', 'II', 'III', 'IV', 'V', 'VI', 'VII', 'VIII', 'IX', 'X', 'XI', 'XII']:\n",
    "            if current_product:\n",
    "                result.append(current_product)\n",
    "            \n",
    "            current_product = {\n",
    "                \"ten_san_pham\": hang_hoa,\n",
    "                \"cac_muc\": []\n",
    "            }\n",
    "            current_category = None\n",
    "            \n",
    "        # Nếu STT là số (1, 2, 3...) thì đây là danh mục\n",
    "        elif stt.isdigit():\n",
    "            current_category = {\n",
    "                \"ten_hang_hoa\": hang_hoa,\n",
    "                \"thong_so_ky_thuat\": {}\n",
    "            }\n",
    "            \n",
    "            # Xử lý yêu cầu kỹ thuật cho danh mục\n",
    "            if yeu_cau.strip():\n",
    "                requirements = split_requirements(yeu_cau)\n",
    "                for req in requirements:\n",
    "                    key = generate_random_key()  # Tạo key random\n",
    "                    current_category[\"thong_so_ky_thuat\"][key] = clean_text(req)\n",
    "            if current_product:\n",
    "                current_product[\"cac_muc\"].append(current_category)\n",
    "                \n",
    "        # Nếu STT trống thì đây là thông số kỹ thuật chi tiết\n",
    "        elif stt == '' and current_category and hang_hoa:\n",
    "            # Tạo key random cho thông số kỹ thuật\n",
    "            key = generate_random_key()\n",
    "            \n",
    "            # Làm sạch tên hàng hóa và yêu cầu kỹ thuật\n",
    "            clean_hang_hoa = clean_text(hang_hoa)\n",
    "            clean_yeu_cau = clean_text(yeu_cau)\n",
    "            \n",
    "            current_category[\"thong_so_ky_thuat\"][key] = [clean_hang_hoa, clean_yeu_cau]\n",
    "        elif stt == '' and current_category and not hang_hoa:\n",
    "            if yeu_cau.strip():\n",
    "                requirements = split_requirements(yeu_cau)\n",
    "                \n",
    "                # Lấy key cuối cùng trong thong_so_ky_thuat (nếu có)\n",
    "                existing_keys = list(current_category[\"thong_so_ky_thuat\"].keys())\n",
    "                last_key = existing_keys[-1] if existing_keys else None\n",
    "                \n",
    "                for req in requirements:\n",
    "                    clean_req = clean_text(req)\n",
    "                    \n",
    "                    # Kiểm tra chữ cái đầu có viết hoa HOẶC có gạch đầu dòng không\n",
    "                    has_dash = req.strip().startswith('- ')\n",
    "                    has_uppercase = clean_req and clean_req[0].isupper()\n",
    "                    \n",
    "                    if has_uppercase or has_dash:\n",
    "                        # Chữ đầu viết hoa HOẶC có gạch đầu dòng -> tạo key mới\n",
    "                        key = generate_random_key()\n",
    "                        current_category[\"thong_so_ky_thuat\"][key] = clean_req\n",
    "                        last_key = key\n",
    "                    else:\n",
    "                        # Chữ đầu không viết hoa VÀ không có gạch đầu dòng -> nối vào key trước đó\n",
    "                        if last_key and last_key in current_category[\"thong_so_ky_thuat\"]:\n",
    "                            current_category[\"thong_so_ky_thuat\"][last_key] += \" \" + clean_req\n",
    "                        else:\n",
    "                            # Nếu không có key trước đó thì vẫn tạo key mới\n",
    "                            key = generate_random_key()\n",
    "                            current_category[\"thong_so_ky_thuat\"][key] = clean_req\n",
    "                            last_key = key\n",
    "    \n",
    "    # Thêm sản phẩm cuối cùng\n",
    "    if current_product:\n",
    "        result.append(current_product)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Chuyển đổi dữ liệu\n",
    "converted_data = convert_to_new_format(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a80b686-2703-469f-8249-266a1966c7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b53020a-ee0f-4a69-b00f-14b6e5a77b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_prompts = []\n",
    "for item in converted_data:\n",
    "    ten_san_pham = item['ten_san_pham']\n",
    "    for muc in item['cac_muc']:\n",
    "        ten_hang_hoa = muc['ten_hang_hoa']\n",
    "        thong_so_ky_thuat = muc['thong_so_ky_thuat']\n",
    "        for key, value in thong_so_ky_thuat.items():\n",
    "            if isinstance(value, list):\n",
    "                value_str = ' '.join(value)\n",
    "            else:\n",
    "                value_str = value\n",
    "            query = {\n",
    "                key: f\"{ten_san_pham} {ten_hang_hoa} {value_str}\"\n",
    "            }\n",
    "            context_prompts.append(query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a9f9a565-44c8-45bd-88d7-469cafccc6fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "12",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mquery\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m12\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[31mKeyError\u001b[39m: 12"
     ]
    }
   ],
   "source": [
    "print(query[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0f9582-c3e3-482a-b750-03b645fc5c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04023546-5ffb-4d50-b6da-a399cdbde6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_rag(url: str, api_key: str, query: str, collection_name: str = \"thong_tin_san_pham\", file_name: str = \"NetSure_732_User_Manual\"):\n",
    "    # Cấu hình client Qdrant\n",
    "    client = QdrantClient(\n",
    "        url=url,\n",
    "        api_key=api_key,\n",
    "    )\n",
    "    aclient = AsyncQdrantClient(\n",
    "        url=url,\n",
    "        api_key=api_key,\n",
    "    )\n",
    "    # Khởi tạo Vector Store\n",
    "    vector_store = QdrantVectorStore(\n",
    "        collection_name=collection_name,\n",
    "        client=client,\n",
    "        aclient=aclient,\n",
    "    )\n",
    "    filters = MetadataFilters(\n",
    "        filters=[\n",
    "            MetadataFilter(key=\"file_name\", operator=FilterOperator.EQ, value=file_name),\n",
    "            MetadataFilter(key=\"type\", operator=FilterOperator.EQ, value=\"chunk_document\"),\n",
    "        ],\n",
    "        condition=FilterCondition.AND,\n",
    "    )\n",
    "\n",
    "    index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n",
    "\n",
    "    retriever = index.as_retriever(similarity_top_k=5, verbose=True, filters=filters)\n",
    "\n",
    "    query = query\n",
    "\n",
    "    # --- Thay đổi từ đây ---\n",
    "    query_engine = RetrieverQueryEngine.from_args(\n",
    "        retriever=retriever\n",
    "    )\n",
    "\n",
    "    # 2. Thực hiện truy vấn qua Query Engine\n",
    "    print(\"Bắt đầu truy vấn với Query Engine...\")\n",
    "    response = query_engine.retrieve(query)\n",
    "    text_content = \"\"\n",
    "    for node in response:\n",
    "        text_content += node.get_content() + \"\\n\"\n",
    "    return text_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1637b073-62cc-4fc6-b6e4-00a6a9028f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for query in queries:\n",
    "    for key, value in query.items():\n",
    "        vector = search_rag(value)\n",
    "        results.append({\n",
    "            \"key\": key,\n",
    "            \"value\": value,\n",
    "            \"text_content\": text_content  \n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad51766d-f666-47b8-a4ec-842d26ee6f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query(requirement: str, text_content: str):\n",
    "    template = (\n",
    "        \"Based on the following text, create a comprehensive summary for the entire document.\\n\"\n",
    "        \"Document:\\n---\\n{requirement}\\n {text_content}---\\nSummary:\"\n",
    "    )\n",
    "    prompt_template = PromptTemplate(template)\n",
    "    response = Settings.llm.predict(prompt_template, requirement=requirement, text_content=text_content)\n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8538e0e-24c1-44e0-b254-576fd521a2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = []\n",
    "for item in results:\n",
    "    value = item[\"value\"]\n",
    "    vector = item[\"vector\"]\n",
    "    answer = create_query(value, text_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
